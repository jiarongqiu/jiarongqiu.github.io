---
layout: post
title: 机器学习第四章(p73-p95)
categories: [Computer Science]
tags: [ML]
---

### 4.1 基本流程(p73)
三种递归返回情况:

1. 样本中全部为同一类别，将其标记为叶子节点，返回。
2. 样本中属性值全部相同，或者均无属性值，标记为叶子节点，类别为数量出现最多的类别，返回。
3. 样本集为空，将其标记为叶子节点，类别为父节点出现最多的类别，返回。

基本算法：
1. 考虑递归返回情形1与情形2。
2. 选择一个最优划分属性，根据属性值划分为多个分支。
3. 对每一个分支，考虑分支样本集的递归返回情形3。如果没有，递归决策树算法。
4. 输出以node为跟原点的决策树。

### 4.2 划分选择(p75)
分支样本尽可能属于同一类别，纯度(purity)越来越高。
#### 4.2.1 信息增益
* 信息熵(information entropy) $$Ent(D) = - \sum_{k=1}^{y}{p_k log_2p_k}$$
* 信息增益(information gain) $$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}{\frac{D^v}{D}Ent(D^v)}$$
* ID3算法就是以信息增益最大为划分标准,但以此为标准的划分会倾向于属性值较多的属性，即分支更多。

#### 4.2.2 增益率
* 为解决上述问题，C4.5算法使用增益率 $$GainRatio(D,a) = \frac{Gain(D,a)}{IV(a)} $$，其中$$IV(a) = -\sum_{v=1}^{V} {\frac{D^v}{D} log_2 \frac{D^v}{D}}$$

#### 4.2.3 基尼系数
* 在CART决策树中，选择基尼指数来选择划分属性，反映的是从数据集D中抽取两个样本，不一致的概率。值越小，纯度越高。$$Gini(D) = \sum_{k=1}^{y}{\sum_{k' \neq k}{p_k p_{k'}}}$$

### 4.3 剪枝处理(p79)

#### 4.3.1 预剪枝
每一次划分都在验证集上验证，若有在验证集上精度有所提升，则允许当前划分。减少了计算开销，提升了泛化能力，但其贪心的本质导致决策树有欠拟合的风险。

#### 4.3.2 后剪枝
后剪枝类似上述方式，只不过是在生成完整决策树后，根据验证集对原树进行裁剪，往往比预剪枝的决策树要大，欠拟合的风险小。

### 4.4 连续与缺失值

#### 4.4.1 连续值处理
* 二分法，取划分为两个连续值的中间值，然后寻找最优划分。

#### 4.4.2 缺失值处理
对于属性a, D'为无缺失样本子集，$$\rho$$ 表示无缺失值样本所占比例，$$p_k$$代表无缺失样本中各类别比例，$$r_v$$代表无缺失样本中各属性占比，则可以推广信息增益公式为

$$ 
\begin{align}
Gain(D,a) &= \rho * Gain(D',a)\\
		  &= \rho * (Ent(D')-\sum_{v=1}^{V}{r_v Ent(D'^v)})\\
\end{align}
$$

其中,$$Ent(D') = - \sum_{k=1}^{K}{p_k log_2 p_k} $$。

### 4.5 多变量决策树
实现在空间中斜着划分，可以用线性分类器，相较单变量决策树更为简化。
